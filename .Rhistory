library("ARI")
install.packages("ARI", dependencies = TRUE)
install.packages("ari", dependencies = TRUE)
lambdas <- c("10000.0", "1000.0", "100.0", "10.0", "1.0", "0.1", "0.01", "0.001", "0.0001", "1e-05", "0.0")
lambdas <- rev(lambdas)
cos_nmi <- c()
cos_ari <- c()
for(lambda in lambdas){
#file <- paste("result_wc_nmtf/lambda/cos/wc-nmtf_Z_l", lambda, ".csv", sep = "")
file <- paste("result_wc_nmtf/lambda/p_tra/wc-nmtf_Z_l", lambda, "_p_tra.csv", sep = "")
res_wc_nmtf <- read.csv(file, header = TRUE)
res_wc_nmtf <- t( normalize( t(res_wc_nmtf) ) )
label_res <- apply(res_wc_nmtf, MARGIN = 1, FUN=which.max)
t_nmi <- NMI(label_res, labelK)
t_ari <- ARI(label_res, labelK)
cos_nmi <- c(cos_nmi, t_nmi)
cos_ari <- c(cos_ari, t_ari)
}
for(lambda in lambdas){
#file <- paste("result_wc_nmtf/lambda/cos/wc-nmtf_Z_l", lambda, ".csv", sep = "")
file <- paste("result_wc_nmtf/lambda/p_tra/wc-nmtf_Z_l", lambda, "_p_tra.csv", sep = "")
res_wc_nmtf <- read.csv(file, header = TRUE)
res_wc_nmtf <- t( normalize( t(res_wc_nmtf) ) )
label_res <- apply(res_wc_nmtf, MARGIN = 1, FUN=which.max)
t_nmi <- NMI(label_res, labelK)
t_ari <- ARI(label_res, labelK)
cos_nmi <- c(cos_nmi, t_nmi)
cos_ari <- c(cos_ari, t_ari)
}
temp = c(1:10)
matrix(c, nrow = 5)
matrix(temp, nrow = 5)
t <- Sys.time()
t2 <- Sys.time()
t2 - t
temp <- t2 - t
temp
plot(temp)
?dist
?svg
setwd("/media/matthieu/Data/Matthieu/##Etude/#M1/S2/BD2/opinion-mining")
library(aricode)
#library(NMI)
library(R.matlab)
wcnmtf_res = read.csv("result/tf_idf_wc-nmtf_Z_l1.0.csv")
wcnmtf_res = read.csv("result/tf_idf_wc-nmtf_Z_l1.0.csv")
wcnmtf_cluster = apply(wcnmtf_res, MARGIN = 1, FUN = which.max)
write.csv(wcnmtf_cluster, "result/tf_idf_wc_nmtf_clusters.csv")
#wcnmtf_cluster = read.csv("result/doc2vec_wc_nmtf_clusters.csv", row.names = 1)
wcnmtf_cluster = read.csv("result/tf_idf_wc_nmtf_clusters.csv", row.names = 1)
dim(wcnmtf_cluster)
wcnmtf_ind1 = which(wcnmtf_cluster == 1)
wcnmtf_ind2 = which(wcnmtf_cluster == 2)
length(wcnmtf_cluster[wcnmtf_ind1,])
length(wcnmtf_cluster[wcnmtf_ind2,])
nmtf_c <- unlist(wcnmtf_cluster, use.names=FALSE)
lexicon_cluster <- read.csv("result/tweets_clean10.csv_lexicon_sentiment_pos_neg.csv", row.names = 1)
length(lexicon_cluster[lexicon_cluster == 1])
length(lexicon_cluster[lexicon_cluster == 0])
l_c <- unlist(lexicon_cluster, use.names=FALSE)
NMI(nmtf_c, l_c)
ARI(nmtf_c, l_c)
kmeans_cluster = read.csv("result/dm_kmeans_clusters.csv", row.names = 1)
kmeans_ind1 = which(kmeans_cluster == 1)
kmeans_ind2 = which(kmeans_cluster == 2)
length(kmeans_cluster[kmeans_ind1,])
length(kmeans_cluster[kmeans_ind2,])
k_c <- unlist(kmeans_cluster, use.names=FALSE)
NMI(k_c, l_c)
ARI(k_c, l_c)
skmeans_cluster = read.csv("result/dm_skmeans_clusters.csv", row.names = 1)
skmeans_ind1 = which(skmeans_cluster == 1)
skmeans_ind2 = which(skmeans_cluster == 2)
length(skmeans_cluster[skmeans_ind1,])
length(skmeans_cluster[skmeans_ind2,])
NMI(sk_c, l_c)
ARI(sk_c, l_c)
sk_c <- unlist(skmeans_cluster, use.names=FALSE)
NMI(sk_c, l_c)
ARI(sk_c, l_c)
skmeans_cluster = read.csv("result/dm_skmeans_clusters.csv", row.names = 1)
skmeans_ind1 = which(skmeans_cluster == 1)
skmeans_ind2 = which(skmeans_cluster == 2)
length(skmeans_cluster[skmeans_ind1,])
length(skmeans_cluster[skmeans_ind2,])
kmeans_cluster = read.csv("result/dm_kmeans_clusters.csv", row.names = 1)
kmeans_ind1 = which(kmeans_cluster == 1)
kmeans_ind2 = which(kmeans_cluster == 2)
length(kmeans_cluster[kmeans_ind1,])
length(kmeans_cluster[kmeans_ind2,])
#wcnmtf_cluster = read.csv("result/doc2vec_wc_nmtf_clusters.csv", row.names = 1)
wcnmtf_cluster = read.csv("result/tf_idf_wc_nmtf_clusters.csv", row.names = 1)
wcnmtf_ind1 = which(wcnmtf_cluster == 1)
wcnmtf_ind2 = which(wcnmtf_cluster == 2)
length(wcnmtf_cluster[wcnmtf_ind1,])
length(wcnmtf_cluster[wcnmtf_ind2,])
setwd("/media/matthieu/Data/Matthieu/##Etude/#M1/S2/BD2/opinion-mining")
library(aricode)
library(R.matlab)
library(skmeans)
normalize <- function(x) {x / sqrt(rowSums(x^2))}
normalizeByCol <- function(df) { t( normalize( t(df) ) )}
sent_process <- function(x){ x[1] - x[2] + 1 }
df <- read.csv("dataset/tweets_clean10.csv_doc2Vec_50.csv", header = TRUE, row.names = 1)
dim(df)
mat_df <- as.matrix(df)
mat_df <- normalize(mat_df)
dim(mat_df)
k <- 2
print("run spherical kmeans...")
res_skmeans <- skmeans(mat_df, k, control = list(verbose = TRUE), method = "pclust")
write.csv(res_skmeans$cluster, "doc2vec_skmeans_clusters.csv")
