library("ARI")
install.packages("ARI", dependencies = TRUE)
install.packages("ari", dependencies = TRUE)
lambdas <- c("10000.0", "1000.0", "100.0", "10.0", "1.0", "0.1", "0.01", "0.001", "0.0001", "1e-05", "0.0")
lambdas <- rev(lambdas)
cos_nmi <- c()
cos_ari <- c()
for(lambda in lambdas){
#file <- paste("result_wc_nmtf/lambda/cos/wc-nmtf_Z_l", lambda, ".csv", sep = "")
file <- paste("result_wc_nmtf/lambda/p_tra/wc-nmtf_Z_l", lambda, "_p_tra.csv", sep = "")
res_wc_nmtf <- read.csv(file, header = TRUE)
res_wc_nmtf <- t( normalize( t(res_wc_nmtf) ) )
label_res <- apply(res_wc_nmtf, MARGIN = 1, FUN=which.max)
t_nmi <- NMI(label_res, labelK)
t_ari <- ARI(label_res, labelK)
cos_nmi <- c(cos_nmi, t_nmi)
cos_ari <- c(cos_ari, t_ari)
}
for(lambda in lambdas){
#file <- paste("result_wc_nmtf/lambda/cos/wc-nmtf_Z_l", lambda, ".csv", sep = "")
file <- paste("result_wc_nmtf/lambda/p_tra/wc-nmtf_Z_l", lambda, "_p_tra.csv", sep = "")
res_wc_nmtf <- read.csv(file, header = TRUE)
res_wc_nmtf <- t( normalize( t(res_wc_nmtf) ) )
label_res <- apply(res_wc_nmtf, MARGIN = 1, FUN=which.max)
t_nmi <- NMI(label_res, labelK)
t_ari <- ARI(label_res, labelK)
cos_nmi <- c(cos_nmi, t_nmi)
cos_ari <- c(cos_ari, t_ari)
}
temp = c(1:10)
matrix(c, nrow = 5)
matrix(temp, nrow = 5)
t <- Sys.time()
t2 <- Sys.time()
t2 - t
temp <- t2 - t
temp
plot(temp)
?dist
?svg
df <- read.csv("dataset/tweets_clean10.csv_doc2Vec.csv", header = TRUE, row.names = 1)
setwd("/media/matthieu/Data/Matthieu/##Etude/#M1/S2/BD2/opinion-mining")
library(aricode)
library(R.matlab)
library(skmeans)
normalize <- function(x) {x / sqrt(rowSums(x^2))}
normalizeByCol <- function(df) { t( normalize( t(df) ) )}
sent_process <- function(x){ x[1] - x[2] + 1 }
df <- read.csv("dataset/tweets_clean10.csv_doc2Vec.csv", header = TRUE, row.names = 1)
dim(df)
mat_df <- as.matrix(df)
mat_df <- normalize(mat_df)
dim(mat_df)
res <- kmeans(mat_df, centers = k)
k <- 2
print("run kmeans...")
res <- kmeans(mat_df, centers = k)
print("run spherical kmeans...")
res2 <- skmeans(mat_df, k)
res2 <- skmeans(mat_df, k, verbose = TRUE)
res2 <- skmeans(mat_df, k, control = list(verbose = TRUE))
setwd("/media/matthieu/Data/Matthieu/##Etude/#M1/S2/BD2/opinion-mining")
lexicon_sent <- read.csv("dataset/tweets_clean10.csv_lexicon_sentiment_pos_neg.csv")
lexicon_sent <- read.csv("results/tweets_clean10.csv_lexicon_sentiment_pos_neg.csv")
n_pos <- length(lexicon_sent[lexicon_sent == 1])
n_neg <- length(lexicon_sent[lexicon_sent == 0])
library("cluster")
silhouette(lexicon_sent)
lexicon_sent
length(lexicon_sent)
dim(lexicon_sent)
lexicon_sent <- read.csv("results/tweets_clean10.csv_lexicon_sentiment_pos_neg.csv", row.names = 1)
length(lexicon_sent)
lexicon_sent
head(lexicon_sent)
?read.csv
lexicon_sent <- read.csv("results/tweets_clean10.csv_lexicon_sentiment_pos_neg.csv", row.names = 1)
dim(lexicon_sent)
list(lexicon_sent)
silhouette(list(lexion_sent))
silhouette(list(lexicon_sent))
setwd("/media/matthieu/Data/Matthieu/##Etude/#M1/S2/BD2/opinion-mining")
library(aricode)
library(R.matlab)
library(skmeans)
normalize <- function(x) {x / sqrt(rowSums(x^2))}
normalizeByCol <- function(df) { t( normalize( t(df) ) )}
sent_process <- function(x){ x[1] - x[2] + 1 }
df <- read.csv("dataset/tweets_clean10.csv_doc2Vec.csv", header = TRUE, row.names = 1)
dim(df)
mat_df <- as.matrix(df)
mat_df <- normalize(mat_df)
dim(mat_df)
res <- kmeans(mat_df, centers = k)
res$cluster
typeof(res$cluster)
silhouette(res$cluster)
setwd("/media/matthieu/Data/Matthieu/##Etude/#M1/S2/BD2/opinion-mining")
library(aricode)
library(R.matlab)
library(skmeans)
library(cluster)
normalize <- function(x) {x / sqrt(rowSums(x^2))}
normalizeByCol <- function(df) { t( normalize( t(df) ) )}
sent_process <- function(x){ x[1] - x[2] + 1 }
df <- read.csv("dataset/tweets_clean10.csv_doc2Vec.csv", header = TRUE, row.names = 1)
dim(df)
mat_df <- as.matrix(df)
mat_df <- normalize(mat_df)
dim(mat_df)
k <- 2
print("run kmeans...")
res_kmeans <- kmeans(mat_df, centers = k)
silhouette(kmeans_cluster$cluster, dist(mat_df))
silhouette(res_kmeans$cluster, dist(mat_df))
dist(mat_df)
lexicon_sent <- read.csv("results/tweets_clean10.csv_lexicon_sentiment_pos_neg.csv", row.names = 1)
n_pos <- length(lexicon_sent[lexicon_sent == 1])
n_neg <- length(lexicon_sent[lexicon_sent == 0])
?kmeans
wc_nmtf <- read.csv("results/wc-nmtf_Z_l0.0.csv")
dim(wc_nmtf)
wc_nmtf <- apply(wc_nmtf, FUN = which.max)
wc_nmtf <- apply(wc_nmtf, MARGIN = 1, FUN = which.max)
dim(wc_nmtf)
length(wc_nmtf)
wc_nmtf
write.csv(wc_nmtf, "results/tweets_clean10.csv_doc2vec_wc_nmtf.csv")
wcnmtf_cluster = read.csv("results/doc2vec_wc_nmtf_clusters.csv")
wcnmtf_ind1 = which(wcnmtf_cluster == 1)
wcnmtf_ind2 = which(wcnmtf_cluster == 2)
length(wcnmtf_cluster[wcnmtf_ind1])
length(wcnmtf_cluster[wcnmtf_ind2])
skmeans_cluster = read.csv("results/doc2vec_skmeans_clusters.csv")
skmeans_ind1 = which(skmeans_cluster == 1)
skmeans_ind2 = which(skmeans_cluster == 2)
length(skmeans_cluster[skmeans_ind1])
length(skmeans_cluster[skmeans_ind2])
skmeans_cluster
skmeans_cluster = read.csv("results/doc2vec_skmeans_clusters.csv", row.names = 1)
skmeans_ind1 = which(skmeans_cluster == 1)
skmeans_ind2 = which(skmeans_cluster == 2)
length(skmeans_cluster[skmeans_ind1])
length(skmeans_cluster[skmeans_ind2])
dim(skmeans_cluster)
skmeans_cluster = read.csv("results/doc2vec_skmeans_clusters.csv", row.names = 1)
skmeans_ind1 = which(skmeans_cluster == 1)
skmeans_ind2 = which(skmeans_cluster == 2)
length(skmeans_cluster[skmeans_ind1,])
length(skmeans_cluster[skmeans_ind2,])
kmeans_cluster = read.csv("results/doc2vec_kmeans_clusters.csv")
dim(kmeans_cluster)
wcnmtf_cluster = read.csv("results/doc2vec_wc_nmtf_clusters.csv", row.names = 1)
wcnmtf_ind1 = which(wcnmtf_cluster == 1)
wcnmtf_ind2 = which(wcnmtf_cluster == 2)
length(wcnmtf_cluster[wcnmtf_ind1,])
length(wcnmtf_cluster[wcnmtf_ind2,])
